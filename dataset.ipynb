{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read initial data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv = 'volodymyr_graphs/graph_test_1.csv'\n",
    "\n",
    "data = pd.read_csv(path_to_csv) \n",
    "data = data.sort_values(by='modularity_class')\n",
    "data = data.drop(columns=['timeset'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query summaries and titles###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['id', 'title', 'extract'])\n",
    "\n",
    "# Ids from the initial graph\n",
    "ids = sorted(data.Id.tolist())\n",
    "\n",
    "for idd in ids:\n",
    "    # Change 'en' to 'fr' for french , to 'ar' for arabic \n",
    "    base_url = 'https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro&explaintext&redirects=1&pageids='\n",
    "    url = base_url + str(idd)\n",
    "    res = requests.get(url)\n",
    "    response = json.loads(res.text)\n",
    "    for key in response['query']['pages']:\n",
    "        try:\n",
    "            pageid = response['query']['pages'][key]['pageid']\n",
    "            title = response['query']['pages'][key]['title']\n",
    "            extract = response['query']['pages'][key]['extract']\n",
    "            \n",
    "            df = df.append({'id': pageid, 'title': title, 'extract': extract}, ignore_index=True)\n",
    "        except KeyError as e:\n",
    "            print(e, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save it for later :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_extracts = path_to_csv.split('.')[0]+ '_extracts.' + path_to_csv.split('.')[1]\n",
    "df.to_csv(path_to_extracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_e = pd.read_csv(path_to_extracts) #dataset with scraped extracts (summary of page)from web \n",
    "\n",
    "df_final = data.join(data_e.set_index('id'), on='Id')\n",
    "df_final = df_final.dropna()\n",
    "df_final = df_final.drop(columns=['Label'])\n",
    "df_final = df_final.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "df_final.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
