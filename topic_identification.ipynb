{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/graph_test.csv') #dataset with title\n",
    "data = data.sort_values(by='modularity_class')\n",
    "data = data.drop(columns=['timeset'])\n",
    "\n",
    "data_e = pd.read_csv('data/extracts.csv') #dataset with scraped extracts (summary of page)from web \n",
    "data_t = pd.read_csv('data/texts.csv') #dataset with scraped extracts (texts of page)from web\n",
    "\n",
    "df = data.join(data_e.set_index('id'), on='Id')\n",
    "df = df.join(data_t.set_index('id'), on='Id')\n",
    "df = df.dropna()\n",
    "df = df.drop(columns=['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>modularity_class</th>\n",
       "      <th>title</th>\n",
       "      <th>extract</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>427412</td>\n",
       "      <td>0</td>\n",
       "      <td>Saint-Nazaire</td>\n",
       "      <td>Saint-Nazaire (French pronunciation: ​[sɛ̃.na....</td>\n",
       "      <td>&lt;p class=\"mw-empty-elt\"&gt;\\n&lt;/p&gt;\\n&lt;p&gt;&lt;b&gt;Saint-Na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>152639</td>\n",
       "      <td>0</td>\n",
       "      <td>Robert Stewart, Viscount Castlereagh</td>\n",
       "      <td>Robert Stewart, 2nd Marquess of Londonderry,  ...</td>\n",
       "      <td>&lt;p class=\"mw-empty-elt\"&gt;\\n\\n&lt;/p&gt;\\n&lt;p&gt;&lt;b&gt;Robert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>46383</td>\n",
       "      <td>0</td>\n",
       "      <td>Jane Seymour</td>\n",
       "      <td>Jane Seymour (c. 1508 – 24 October 1537) was Q...</td>\n",
       "      <td>&lt;p class=\"mw-empty-elt\"&gt;\\n&lt;/p&gt;\\n\\n&lt;p&gt;&lt;b&gt;Jane S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1057856</td>\n",
       "      <td>0</td>\n",
       "      <td>Jahangir Khan</td>\n",
       "      <td>Jahangir Khan, HI (Pashto / Urdu: جهانگير خان‎...</td>\n",
       "      <td>&lt;p class=\"mw-empty-elt\"&gt;\\n\\n&lt;/p&gt;\\n&lt;p&gt;&lt;b&gt;Jahang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>75795</td>\n",
       "      <td>0</td>\n",
       "      <td>Juan Carlos I of Spain</td>\n",
       "      <td>Juan Carlos I (Spanish: [xwaŋˈkaɾlos]; Juan Ca...</td>\n",
       "      <td>&lt;p class=\"mw-empty-elt\"&gt;\\n&lt;/p&gt;\\n\\n&lt;p&gt;&lt;b&gt;Juan C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id  modularity_class                                 title  \\\n",
       "176   427412                 0                         Saint-Nazaire   \n",
       "396   152639                 0  Robert Stewart, Viscount Castlereagh   \n",
       "541    46383                 0                          Jane Seymour   \n",
       "131  1057856                 0                         Jahangir Khan   \n",
       "428    75795                 0                Juan Carlos I of Spain   \n",
       "\n",
       "                                               extract  \\\n",
       "176  Saint-Nazaire (French pronunciation: ​[sɛ̃.na....   \n",
       "396  Robert Stewart, 2nd Marquess of Londonderry,  ...   \n",
       "541  Jane Seymour (c. 1508 – 24 October 1537) was Q...   \n",
       "131  Jahangir Khan, HI (Pashto / Urdu: جهانگير خان‎...   \n",
       "428  Juan Carlos I (Spanish: [xwaŋˈkaɾlos]; Juan Ca...   \n",
       "\n",
       "                                                  text  \n",
       "176  <p class=\"mw-empty-elt\">\\n</p>\\n<p><b>Saint-Na...  \n",
       "396  <p class=\"mw-empty-elt\">\\n\\n</p>\\n<p><b>Robert...  \n",
       "541  <p class=\"mw-empty-elt\">\\n</p>\\n\\n<p><b>Jane S...  \n",
       "131  <p class=\"mw-empty-elt\">\\n\\n</p>\\n<p><b>Jahang...  \n",
       "428  <p class=\"mw-empty-elt\">\\n</p>\\n\\n<p><b>Juan C...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return (l[i:i+n] for i in range(0, len(l), n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete the dataset - web scraping ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "\n",
    "# df = pd.DataFrame(columns=['id', 'title', 'extract'])\n",
    "\n",
    "# ids = sorted(data.Id.tolist())\n",
    "# id_chunks = chunks(ids, 49)\n",
    "# for chunk in id_chunks:\n",
    "#     string_ids = \"\"\n",
    "#     for idd in chunk:\n",
    "#         string_ids = string_ids + str(idd) + '|'\n",
    "#     base_url = 'https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro&explaintext&redirects=1&pageids='\n",
    "#     url = base_url + string_ids[:-1]\n",
    "#     res = requests.get(url)\n",
    "#     response = json.loads(res.text)\n",
    "#     print(response)\n",
    "#     for key in response['query']['pages']:\n",
    "#         try:\n",
    "#             pageid = response['query']['pages'][key]['pageid']\n",
    "#             title = response['query']['pages'][key]['title']\n",
    "#             extract = response['query']['pages'][key]['extract']\n",
    "#             df = df.append({'id': pageid, 'title': title, 'extract': extract}, ignore_index=True)\n",
    "#         except KeyError as e:\n",
    "#             pass\n",
    "# #             print(e, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "\n",
    "# df = pd.DataFrame(columns=['id', 'title', 'extract'])\n",
    "\n",
    "# ids = sorted(data.Id.tolist())\n",
    "# for idd in ids:\n",
    "#     base_url = 'https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro&explaintext&redirects=1&pageids='\n",
    "#     url = base_url + str(idd)\n",
    "#     res = requests.get(url)\n",
    "#     response = json.loads(res.text)\n",
    "#     for key in response['query']['pages']:\n",
    "#         try:\n",
    "#             pageid = response['query']['pages'][key]['pageid']\n",
    "#             title = response['query']['pages'][key]['title']\n",
    "#             extract = response['query']['pages'][key]['extract']\n",
    "#             df = df.append({'id': pageid, 'title': title, 'extract': extract}, ignore_index=True)\n",
    "#         except KeyError as e:\n",
    "#             print(e, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "\n",
    "# df = pd.DataFrame(columns=['id', 'text'])\n",
    "\n",
    "# ids = sorted(data.Id.tolist())\n",
    "# for idd in ids:\n",
    "#     base_url = 'https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&redirects=true&pageids='\n",
    "#     #base_url = 'https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro&explaintext&redirects=1&pageids='\n",
    "#     url = base_url + str(idd)\n",
    "#     res = requests.get(url)\n",
    "#     response = json.loads(res.text)\n",
    "#     for key in response['query']['pages']:\n",
    "#         try:\n",
    "#             pageid = response['query']['pages'][key]['pageid']\n",
    "#             texts = response['query']['pages'][key]['extract']\n",
    "#             df = df.append({'id': pageid, 'text': texts}, ignore_index=True)\n",
    "#         except KeyError as e:\n",
    "#             print(e, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_title = df.groupby(\"modularity_class\")['title'].apply(' '.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_extract = df.groupby(\"modularity_class\")['extract'].apply(' '.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modularity_class</th>\n",
       "      <th>extract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Saint-Nazaire (French pronunciation: ​[sɛ̃.na....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Gadsby is a 1939 novel by Ernest Vincent Wrigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Bombshell is a 1933 American pre-Code romantic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Patrick Michael Shanahan (born June 27, 1962) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>West Side Story is a 1961 American romantic mu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   modularity_class                                            extract\n",
       "0                 0  Saint-Nazaire (French pronunciation: ​[sɛ̃.na....\n",
       "1                 1  Gadsby is a 1939 novel by Ernest Vincent Wrigh...\n",
       "2                 2  Bombshell is a 1933 American pre-Code romantic...\n",
       "3                 3  Patrick Michael Shanahan (born June 27, 1962) ...\n",
       "4                 4  West Side Story is a 1961 American romantic mu..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_extract.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Apply a first round of text cleaning techniques\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F\\x80-\\xFF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF]', u'', text)  #remove non-latin chars \n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saintnazaire french pronunciation  breton sant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gadsby is a  novel by ernest vincent wright wr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bombshell is a  american precode romantic come...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patrick michael shanahan born june   is an ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>west side story is a  american romantic musica...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             extract\n",
       "0  saintnazaire french pronunciation  breton sant...\n",
       "1  gadsby is a  novel by ernest vincent wright wr...\n",
       "2  bombshell is a  american precode romantic come...\n",
       "3  patrick michael shanahan born june   is an ame...\n",
       "4  west side story is a  american romantic musica..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean = pd.DataFrame(grouped_extract.extract.apply(round1))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle it for later use\n",
    "data_clean.to_pickle(\"data_clean.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aardvark</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>ababa</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abba</th>\n",
       "      <th>abbas</th>\n",
       "      <th>abbasid</th>\n",
       "      <th>abbassia</th>\n",
       "      <th>...</th>\n",
       "      <th>ögedei</th>\n",
       "      <th>āṣifat</th>\n",
       "      <th>īsā</th>\n",
       "      <th>ōban</th>\n",
       "      <th>ḥizb</th>\n",
       "      <th>ḥiṣān</th>\n",
       "      <th>ḥusayn</th>\n",
       "      <th>ḥusnī</th>\n",
       "      <th>ḥusēn</th>\n",
       "      <th>ḵāšuqjī</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows × 13248 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aardvark  aaron  ab  ababa  abandon  abandoned  abba  abbas  abbasid  \\\n",
       "0          0      0   0      0        0          0     0      0        0   \n",
       "1          0      1   1      0        0          0     0      0        0   \n",
       "2          0      0   0      0        0          0     0      0        0   \n",
       "3          0      0   0      0        0          0     0      0        0   \n",
       "4          0      0   0      0        0          0     0      0        0   \n",
       "5          0      0   0      1        0          0     0      0        0   \n",
       "6          0      0   0      0        0          0     0      0        0   \n",
       "7          0      0   0      0        0          0     0      0        0   \n",
       "8          0      0   0      0        0          0     0      0        0   \n",
       "9          0      0   0      0        0          0     0      0        0   \n",
       "10         0      0   0      0        0          0     0      0        0   \n",
       "11         0      0   0      0        1          0     0      2        0   \n",
       "12         0      0   0      0        0          1     0      0        0   \n",
       "13         0      0   0      0        0          1     0      0        0   \n",
       "14         2      0   0      0        0          0     0      0        0   \n",
       "15         0      0   0      0        0          0     0      0        0   \n",
       "16         0      0   0      0        0          1     1      0        0   \n",
       "17         0      0   0      0        0          0     0      0        1   \n",
       "\n",
       "    abbassia  ...  ögedei  āṣifat  īsā  ōban  ḥizb  ḥiṣān  ḥusayn  ḥusnī  \\\n",
       "0          0  ...       0       0    0     0     0      0       0      0   \n",
       "1          0  ...       0       0    0     0     0      0       0      0   \n",
       "2          0  ...       0       0    0     0     0      0       0      0   \n",
       "3          0  ...       0       0    0     0     0      0       0      0   \n",
       "4          0  ...       0       0    0     0     0      0       0      0   \n",
       "5          0  ...       0       0    0     0     0      0       0      0   \n",
       "6          0  ...       0       0    0     0     0      1       0      0   \n",
       "7          0  ...       0       0    0     0     0      0       0      0   \n",
       "8          0  ...       0       0    0     0     0      0       0      0   \n",
       "9          0  ...       0       0    0     1     0      0       0      0   \n",
       "10         0  ...       0       0    0     0     0      0       0      0   \n",
       "11         0  ...       0       1    0     0     0      0       0      0   \n",
       "12         0  ...       0       0    0     0     0      0       0      0   \n",
       "13         0  ...       0       0    0     0     0      0       0      0   \n",
       "14         0  ...       0       0    0     0     0      0       0      0   \n",
       "15         0  ...       0       0    0     0     0      0       0      0   \n",
       "16         1  ...       0       0    1     0     1      0       1      1   \n",
       "17         0  ...       1       0    0     0     0      0       0      0   \n",
       "\n",
       "    ḥusēn  ḵāšuqjī  \n",
       "0       0        0  \n",
       "1       0        0  \n",
       "2       0        0  \n",
       "3       0        0  \n",
       "4       0        0  \n",
       "5       0        0  \n",
       "6       0        0  \n",
       "7       0        0  \n",
       "8       0        0  \n",
       "9       0        0  \n",
       "10      0        0  \n",
       "11      0        1  \n",
       "12      0        0  \n",
       "13      0        0  \n",
       "14      0        0  \n",
       "15      0        0  \n",
       "16      1        0  \n",
       "17      0        0  \n",
       "\n",
       "[18 rows x 13248 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(data_clean.extract)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = data_clean.index\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle it for later use\n",
    "data_dtm.to_pickle(\"dtm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [('french', 27),\n",
       "  ('edward', 25),\n",
       "  ('battle', 24),\n",
       "  ('france', 23),\n",
       "  ('army', 20),\n",
       "  ('king', 20),\n",
       "  ('order', 17),\n",
       "  ('war', 16),\n",
       "  ('son', 14),\n",
       "  ('juan', 14),\n",
       "  ('second', 13),\n",
       "  ('british', 13),\n",
       "  ('napoleon', 13),\n",
       "  ('blücher', 13),\n",
       "  ('carlos', 12),\n",
       "  ('english', 12),\n",
       "  ('england', 12),\n",
       "  ('members', 11),\n",
       "  ('years', 11),\n",
       "  ('waterloo', 11),\n",
       "  ('military', 11),\n",
       "  ('henry', 11),\n",
       "  ('city', 11),\n",
       "  ('days', 10),\n",
       "  ('paris', 10),\n",
       "  ('coalition', 10),\n",
       "  ('reign', 9),\n",
       "  ('europe', 9),\n",
       "  ('world', 9),\n",
       "  ('toulouse', 9)],\n",
       " 1: [('championship', 31),\n",
       "  ('wwe', 31),\n",
       "  ('united', 31),\n",
       "  ('american', 22),\n",
       "  ('scientology', 22),\n",
       "  ('city', 22),\n",
       "  ('champion', 19),\n",
       "  ('wrestling', 19),\n",
       "  ('professional', 17),\n",
       "  ('title', 17),\n",
       "  ('team', 17),\n",
       "  ('club', 17),\n",
       "  ('league', 16),\n",
       "  ('soccer', 15),\n",
       "  ('church', 14),\n",
       "  ('world', 14),\n",
       "  ('creed', 13),\n",
       "  ('tampa', 13),\n",
       "  ('later', 13),\n",
       "  ('north', 13),\n",
       "  ('states', 13),\n",
       "  ('wrestler', 13),\n",
       "  ('year', 13),\n",
       "  ('tna', 13),\n",
       "  ('area', 12),\n",
       "  ('novel', 12),\n",
       "  ('born', 12),\n",
       "  ('new', 12),\n",
       "  ('known', 12),\n",
       "  ('death', 11)],\n",
       " 2: [('vanderbilt', 88),\n",
       "  ('american', 57),\n",
       "  ('new', 36),\n",
       "  ('family', 32),\n",
       "  ('york', 25),\n",
       "  ('cornelius', 19),\n",
       "  ('known', 19),\n",
       "  ('born', 18),\n",
       "  ('member', 18),\n",
       "  ('jeopardy', 18),\n",
       "  ('century', 17),\n",
       "  ('whitney', 17),\n",
       "  ('city', 15),\n",
       "  ('gloria', 13),\n",
       "  ('states', 12),\n",
       "  ('prominent', 12),\n",
       "  ('william', 12),\n",
       "  ('july', 12),\n",
       "  ('united', 12),\n",
       "  ('film', 11),\n",
       "  ('comedy', 11),\n",
       "  ('trial', 11),\n",
       "  ('john', 11),\n",
       "  ('january', 11),\n",
       "  ('best', 11),\n",
       "  ('jennings', 10),\n",
       "  ('furness', 10),\n",
       "  ('world', 10),\n",
       "  ('october', 10),\n",
       "  ('built', 9)],\n",
       " 3: [('secretary', 18),\n",
       "  ('shanahan', 13),\n",
       "  ('defense', 13),\n",
       "  ('states', 7),\n",
       "  ('acting', 7),\n",
       "  ('united', 7),\n",
       "  ('esper', 7),\n",
       "  ('patrick', 7),\n",
       "  ('army', 6),\n",
       "  ('department', 5),\n",
       "  ('june', 5),\n",
       "  ('trump', 4),\n",
       "  ('confirmed', 3),\n",
       "  ('senate', 3),\n",
       "  ('official', 3),\n",
       "  ('position', 3),\n",
       "  ('president', 3),\n",
       "  ('announced', 3),\n",
       "  ('served', 3),\n",
       "  ('born', 3),\n",
       "  ('september', 3),\n",
       "  ('mark', 2),\n",
       "  ('politician', 2),\n",
       "  ('government', 2),\n",
       "  ('raytheon', 2),\n",
       "  ('office', 2),\n",
       "  ('donald', 2),\n",
       "  ('previously', 2),\n",
       "  ('war', 2),\n",
       "  ('appointed', 1)],\n",
       " 4: [('film', 43),\n",
       "  ('american', 28),\n",
       "  ('films', 28),\n",
       "  ('television', 20),\n",
       "  ('award', 17),\n",
       "  ('role', 17),\n",
       "  ('best', 17),\n",
       "  ('career', 15),\n",
       "  ('musical', 14),\n",
       "  ('born', 14),\n",
       "  ('academy', 14),\n",
       "  ('including', 14),\n",
       "  ('series', 12),\n",
       "  ('director', 12),\n",
       "  ('tisdale', 12),\n",
       "  ('awards', 11),\n",
       "  ('age', 11),\n",
       "  ('actor', 10),\n",
       "  ('lumet', 10),\n",
       "  ('singer', 10),\n",
       "  ('actress', 10),\n",
       "  ('garfield', 9),\n",
       "  ('united', 9),\n",
       "  ('received', 9),\n",
       "  ('began', 9),\n",
       "  ('work', 9),\n",
       "  ('hollywood', 8),\n",
       "  ('producer', 8),\n",
       "  ('death', 8),\n",
       "  ('roles', 8)],\n",
       " 5: [('aircraft', 47),\n",
       "  ('air', 40),\n",
       "  ('fighter', 26),\n",
       "  ('airbus', 24),\n",
       "  ('new', 19),\n",
       "  ('development', 19),\n",
       "  ('systems', 17),\n",
       "  ('united', 14),\n",
       "  ('boeing', 13),\n",
       "  ('bae', 13),\n",
       "  ('company', 12),\n",
       "  ('service', 11),\n",
       "  ('defence', 11),\n",
       "  ('jet', 11),\n",
       "  ('eurofighter', 10),\n",
       "  ('combat', 9),\n",
       "  ('flight', 9),\n",
       "  ('sixthgeneration', 9),\n",
       "  ('family', 9),\n",
       "  ('russian', 8),\n",
       "  ('ethiopian', 8),\n",
       "  ('typhoon', 8),\n",
       "  ('engine', 8),\n",
       "  ('british', 8),\n",
       "  ('dassault', 8),\n",
       "  ('generation', 8),\n",
       "  ('future', 8),\n",
       "  ('version', 8),\n",
       "  ('designed', 7),\n",
       "  ('market', 7)],\n",
       " 6: [('film', 38),\n",
       "  ('love', 28),\n",
       "  ('species', 26),\n",
       "  ('whales', 25),\n",
       "  ('right', 25),\n",
       "  ('whale', 20),\n",
       "  ('united', 17),\n",
       "  ('pacific', 17),\n",
       "  ('mauritius', 16),\n",
       "  ('parakeet', 16),\n",
       "  ('north', 15),\n",
       "  ('women', 14),\n",
       "  ('series', 14),\n",
       "  ('including', 14),\n",
       "  ('population', 13),\n",
       "  ('states', 13),\n",
       "  ('american', 12),\n",
       "  ('family', 12),\n",
       "  ('islands', 12),\n",
       "  ('actress', 12),\n",
       "  ('best', 12),\n",
       "  ('novel', 12),\n",
       "  ('world', 12),\n",
       "  ('arabian', 12),\n",
       "  ('crocodiles', 12),\n",
       "  ('smith', 11),\n",
       "  ('born', 11),\n",
       "  ('known', 11),\n",
       "  ('philosophy', 11),\n",
       "  ('award', 11)],\n",
       " 7: [('series', 40),\n",
       "  ('television', 19),\n",
       "  ('born', 18),\n",
       "  ('film', 17),\n",
       "  ('american', 17),\n",
       "  ('known', 15),\n",
       "  ('comics', 13),\n",
       "  ('dc', 13),\n",
       "  ('boys', 11),\n",
       "  ('new', 9),\n",
       "  ('book', 9),\n",
       "  ('roles', 9),\n",
       "  ('comic', 9),\n",
       "  ('actor', 9),\n",
       "  ('bachelor', 9),\n",
       "  ('hotel', 9),\n",
       "  ('role', 8),\n",
       "  ('marvel', 8),\n",
       "  ('issue', 8),\n",
       "  ('best', 8),\n",
       "  ('model', 7),\n",
       "  ('abc', 7),\n",
       "  ('july', 6),\n",
       "  ('appeared', 6),\n",
       "  ('host', 6),\n",
       "  ('amazon', 6),\n",
       "  ('world', 6),\n",
       "  ('actress', 6),\n",
       "  ('drama', 6),\n",
       "  ('illustrated', 6)],\n",
       " 8: [('game', 55),\n",
       "  ('games', 39),\n",
       "  ('video', 24),\n",
       "  ('released', 20),\n",
       "  ('rare', 18),\n",
       "  ('spectrum', 18),\n",
       "  ('ultimate', 18),\n",
       "  ('graphics', 15),\n",
       "  ('group', 14),\n",
       "  ('computer', 13),\n",
       "  ('series', 12),\n",
       "  ('knight', 12),\n",
       "  ('song', 12),\n",
       "  ('zx', 12),\n",
       "  ('united', 12),\n",
       "  ('wrestling', 12),\n",
       "  ('white', 11),\n",
       "  ('magazine', 11),\n",
       "  ('company', 11),\n",
       "  ('american', 11),\n",
       "  ('including', 11),\n",
       "  ('new', 11),\n",
       "  ('born', 10),\n",
       "  ('isometric', 10),\n",
       "  ('vikernes', 10),\n",
       "  ('later', 10),\n",
       "  ('championship', 10),\n",
       "  ('lore', 9),\n",
       "  ('xbox', 9),\n",
       "  ('bbc', 9)],\n",
       " 9: [('prints', 17),\n",
       "  ('century', 14),\n",
       "  ('printing', 14),\n",
       "  ('ukiyoe', 13),\n",
       "  ('museum', 13),\n",
       "  ('series', 12),\n",
       "  ('british', 12),\n",
       "  ('japanese', 12),\n",
       "  ('art', 11),\n",
       "  ('mirror', 11),\n",
       "  ('games', 10),\n",
       "  ('helmet', 9),\n",
       "  ('east', 9),\n",
       "  ('used', 9),\n",
       "  ('fuji', 8),\n",
       "  ('loot', 8),\n",
       "  ('surface', 8),\n",
       "  ('views', 7),\n",
       "  ('works', 7),\n",
       "  ('known', 7),\n",
       "  ('artists', 7),\n",
       "  ('hiroshiges', 7),\n",
       "  ('mount', 7),\n",
       "  ('woodblock', 7),\n",
       "  ('sutton', 6),\n",
       "  ('western', 6),\n",
       "  ('hokusai', 6),\n",
       "  ('maryon', 6),\n",
       "  ('metalwork', 6),\n",
       "  ('widely', 6)],\n",
       " 10: [('album', 47),\n",
       "  ('american', 39),\n",
       "  ('united', 22),\n",
       "  ('born', 21),\n",
       "  ('released', 20),\n",
       "  ('music', 19),\n",
       "  ('series', 19),\n",
       "  ('band', 18),\n",
       "  ('megadeth', 18),\n",
       "  ('states', 17),\n",
       "  ('sexual', 16),\n",
       "  ('clarkson', 16),\n",
       "  ('number', 15),\n",
       "  ('billboard', 15),\n",
       "  ('studio', 15),\n",
       "  ('security', 14),\n",
       "  ('following', 14),\n",
       "  ('albums', 14),\n",
       "  ('new', 13),\n",
       "  ('women', 13),\n",
       "  ('guitarist', 13),\n",
       "  ('games', 12),\n",
       "  ('metal', 12),\n",
       "  ('hunger', 12),\n",
       "  ('records', 12),\n",
       "  ('federal', 11),\n",
       "  ('grammy', 11),\n",
       "  ('pop', 11),\n",
       "  ('chart', 11),\n",
       "  ('mustaine', 11)],\n",
       " 11: [('united', 45),\n",
       "  ('iran', 44),\n",
       "  ('saudi', 44),\n",
       "  ('states', 39),\n",
       "  ('property', 26),\n",
       "  ('government', 24),\n",
       "  ('iranian', 24),\n",
       "  ('sanctions', 21),\n",
       "  ('president', 21),\n",
       "  ('international', 19),\n",
       "  ('khashoggi', 17),\n",
       "  ('fsln', 15),\n",
       "  ('war', 14),\n",
       "  ('film', 14),\n",
       "  ('waters', 14),\n",
       "  ('including', 14),\n",
       "  ('national', 14),\n",
       "  ('foreign', 14),\n",
       "  ('consulate', 13),\n",
       "  ('irans', 13),\n",
       "  ('american', 12),\n",
       "  ('reagan', 12),\n",
       "  ('gulf', 12),\n",
       "  ('november', 12),\n",
       "  ('october', 12),\n",
       "  ('nicaragua', 12),\n",
       "  ('action', 11),\n",
       "  ('high', 11),\n",
       "  ('contras', 11),\n",
       "  ('persian', 11)],\n",
       " 12: [('american', 26),\n",
       "  ('film', 23),\n",
       "  ('hydrogen', 23),\n",
       "  ('series', 21),\n",
       "  ('television', 16),\n",
       "  ('born', 14),\n",
       "  ('united', 14),\n",
       "  ('comedy', 13),\n",
       "  ('city', 13),\n",
       "  ('states', 13),\n",
       "  ('landis', 12),\n",
       "  ('gas', 12),\n",
       "  ('known', 12),\n",
       "  ('writing', 11),\n",
       "  ('disease', 10),\n",
       "  ('rail', 10),\n",
       "  ('february', 9),\n",
       "  ('murphy', 9),\n",
       "  ('years', 9),\n",
       "  ('directed', 9),\n",
       "  ('based', 8),\n",
       "  ('energy', 8),\n",
       "  ('gases', 8),\n",
       "  ('people', 8),\n",
       "  ('world', 8),\n",
       "  ('actor', 8),\n",
       "  ('new', 8),\n",
       "  ('written', 8),\n",
       "  ('action', 7),\n",
       "  ('dysgraphia', 7)],\n",
       " 13: [('dynasty', 49),\n",
       "  ('chinese', 39),\n",
       "  ('emperor', 32),\n",
       "  ('tang', 29),\n",
       "  ('han', 23),\n",
       "  ('li', 20),\n",
       "  ('nanyue', 20),\n",
       "  ('china', 14),\n",
       "  ('sui', 14),\n",
       "  ('jade', 14),\n",
       "  ('country', 12),\n",
       "  ('zhao', 12),\n",
       "  ('bc', 10),\n",
       "  ('including', 10),\n",
       "  ('population', 10),\n",
       "  ('taizong', 9),\n",
       "  ('central', 9),\n",
       "  ('somalia', 9),\n",
       "  ('regions', 9),\n",
       "  ('international', 8),\n",
       "  ('southern', 8),\n",
       "  ('empire', 8),\n",
       "  ('northern', 8),\n",
       "  ('century', 8),\n",
       "  ('somali', 8),\n",
       "  ('established', 8),\n",
       "  ('languages', 8),\n",
       "  ('nicaragua', 8),\n",
       "  ('kingdom', 7),\n",
       "  ('yang', 7)],\n",
       " 14: [('film', 59),\n",
       "  ('series', 41),\n",
       "  ('actress', 32),\n",
       "  ('american', 31),\n",
       "  ('born', 28),\n",
       "  ('films', 24),\n",
       "  ('awards', 20),\n",
       "  ('television', 20),\n",
       "  ('award', 18),\n",
       "  ('actor', 17),\n",
       "  ('known', 17),\n",
       "  ('roles', 17),\n",
       "  ('best', 15),\n",
       "  ('peewee', 14),\n",
       "  ('reubens', 13),\n",
       "  ('april', 13),\n",
       "  ('role', 13),\n",
       "  ('scoobydoo', 13),\n",
       "  ('comedy', 11),\n",
       "  ('drama', 11),\n",
       "  ('debut', 10),\n",
       "  ('million', 10),\n",
       "  ('media', 10),\n",
       "  ('began', 10),\n",
       "  ('produced', 10),\n",
       "  ('fashion', 9),\n",
       "  ('music', 9),\n",
       "  ('spanish', 9),\n",
       "  ('divergent', 9),\n",
       "  ('including', 9)],\n",
       " 15: [('aircraft', 72),\n",
       "  ('air', 72),\n",
       "  ('force', 40),\n",
       "  ('military', 34),\n",
       "  ('area', 29),\n",
       "  ('states', 28),\n",
       "  ('united', 27),\n",
       "  ('aerial', 25),\n",
       "  ('iran', 25),\n",
       "  ('unmanned', 25),\n",
       "  ('war', 23),\n",
       "  ('command', 22),\n",
       "  ('defence', 21),\n",
       "  ('systems', 20),\n",
       "  ('iranian', 19),\n",
       "  ('control', 18),\n",
       "  ('reconnaissance', 17),\n",
       "  ('soviet', 16),\n",
       "  ('forces', 16),\n",
       "  ('defense', 16),\n",
       "  ('surveillance', 15),\n",
       "  ('raaf', 15),\n",
       "  ('vehicle', 15),\n",
       "  ('flight', 15),\n",
       "  ('operations', 15),\n",
       "  ('used', 15),\n",
       "  ('uavs', 14),\n",
       "  ('islamic', 14),\n",
       "  ('missiles', 14),\n",
       "  ('based', 13)],\n",
       " 16: [('egyptian', 56),\n",
       "  ('president', 55),\n",
       "  ('egypt', 54),\n",
       "  ('morsi', 45),\n",
       "  ('military', 31),\n",
       "  ('june', 27),\n",
       "  ('arabic', 26),\n",
       "  ('minister', 25),\n",
       "  ('election', 25),\n",
       "  ('mohamed', 24),\n",
       "  ('arab', 22),\n",
       "  ('french', 22),\n",
       "  ('war', 21),\n",
       "  ('mubarak', 21),\n",
       "  ('muslim', 21),\n",
       "  ('born', 20),\n",
       "  ('brotherhood', 19),\n",
       "  ('government', 18),\n",
       "  ('served', 18),\n",
       "  ('presidential', 17),\n",
       "  ('prime', 16),\n",
       "  ('political', 16),\n",
       "  ('july', 15),\n",
       "  ('august', 15),\n",
       "  ('egypts', 15),\n",
       "  ('church', 14),\n",
       "  ('france', 13),\n",
       "  ('office', 13),\n",
       "  ('party', 13),\n",
       "  ('coup', 13)],\n",
       " 17: [('film', 39),\n",
       "  ('american', 38),\n",
       "  ('united', 27),\n",
       "  ('states', 25),\n",
       "  ('award', 23),\n",
       "  ('album', 21),\n",
       "  ('released', 19),\n",
       "  ('library', 19),\n",
       "  ('series', 19),\n",
       "  ('known', 18),\n",
       "  ('best', 18),\n",
       "  ('national', 16),\n",
       "  ('born', 16),\n",
       "  ('actor', 15),\n",
       "  ('music', 15),\n",
       "  ('received', 14),\n",
       "  ('books', 14),\n",
       "  ('discworld', 13),\n",
       "  ('world', 13),\n",
       "  ('albums', 13),\n",
       "  ('novels', 12),\n",
       "  ('june', 12),\n",
       "  ('imam', 11),\n",
       "  ('death', 11),\n",
       "  ('nominated', 11),\n",
       "  ('awards', 11),\n",
       "  ('congress', 11),\n",
       "  ('space', 11),\n",
       "  ('television', 11),\n",
       "  ('million', 11)]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the top 30 words in each cluster\n",
    "top_dict = {}\n",
    "for c in data_dtm.transpose().columns:\n",
    "    top = data_dtm.transpose()[c].sort_values(ascending=False).head(30)\n",
    "    top_dict[c]= list(zip(top.index, top.values))\n",
    "\n",
    "top_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['french',\n",
       " 'edward',\n",
       " 'battle',\n",
       " 'france',\n",
       " 'army',\n",
       " 'king',\n",
       " 'order',\n",
       " 'war',\n",
       " 'son',\n",
       " 'juan',\n",
       " 'second',\n",
       " 'british',\n",
       " 'napoleon',\n",
       " 'blücher',\n",
       " 'carlos',\n",
       " 'english',\n",
       " 'england',\n",
       " 'members',\n",
       " 'years',\n",
       " 'waterloo',\n",
       " 'military',\n",
       " 'henry',\n",
       " 'city',\n",
       " 'days',\n",
       " 'paris',\n",
       " 'coalition',\n",
       " 'reign',\n",
       " 'europe',\n",
       " 'world',\n",
       " 'toulouse',\n",
       " 'championship',\n",
       " 'wwe',\n",
       " 'united',\n",
       " 'american',\n",
       " 'scientology',\n",
       " 'city',\n",
       " 'champion',\n",
       " 'wrestling',\n",
       " 'professional',\n",
       " 'title',\n",
       " 'team',\n",
       " 'club',\n",
       " 'league',\n",
       " 'soccer',\n",
       " 'church',\n",
       " 'world',\n",
       " 'creed',\n",
       " 'tampa',\n",
       " 'later',\n",
       " 'north',\n",
       " 'states',\n",
       " 'wrestler',\n",
       " 'year',\n",
       " 'tna',\n",
       " 'area',\n",
       " 'novel',\n",
       " 'born',\n",
       " 'new',\n",
       " 'known',\n",
       " 'death',\n",
       " 'vanderbilt',\n",
       " 'american',\n",
       " 'new',\n",
       " 'family',\n",
       " 'york',\n",
       " 'cornelius',\n",
       " 'known',\n",
       " 'born',\n",
       " 'member',\n",
       " 'jeopardy',\n",
       " 'century',\n",
       " 'whitney',\n",
       " 'city',\n",
       " 'gloria',\n",
       " 'states',\n",
       " 'prominent',\n",
       " 'william',\n",
       " 'july',\n",
       " 'united',\n",
       " 'film',\n",
       " 'comedy',\n",
       " 'trial',\n",
       " 'john',\n",
       " 'january',\n",
       " 'best',\n",
       " 'jennings',\n",
       " 'furness',\n",
       " 'world',\n",
       " 'october',\n",
       " 'built',\n",
       " 'secretary',\n",
       " 'shanahan',\n",
       " 'defense',\n",
       " 'states',\n",
       " 'acting',\n",
       " 'united',\n",
       " 'esper',\n",
       " 'patrick',\n",
       " 'army',\n",
       " 'department',\n",
       " 'june',\n",
       " 'trump',\n",
       " 'confirmed',\n",
       " 'senate',\n",
       " 'official',\n",
       " 'position',\n",
       " 'president',\n",
       " 'announced',\n",
       " 'served',\n",
       " 'born',\n",
       " 'september',\n",
       " 'mark',\n",
       " 'politician',\n",
       " 'government',\n",
       " 'raytheon',\n",
       " 'office',\n",
       " 'donald',\n",
       " 'previously',\n",
       " 'war',\n",
       " 'appointed',\n",
       " 'film',\n",
       " 'american',\n",
       " 'films',\n",
       " 'television',\n",
       " 'award',\n",
       " 'role',\n",
       " 'best',\n",
       " 'career',\n",
       " 'musical',\n",
       " 'born',\n",
       " 'academy',\n",
       " 'including',\n",
       " 'series',\n",
       " 'director',\n",
       " 'tisdale',\n",
       " 'awards',\n",
       " 'age',\n",
       " 'actor',\n",
       " 'lumet',\n",
       " 'singer',\n",
       " 'actress',\n",
       " 'garfield',\n",
       " 'united',\n",
       " 'received',\n",
       " 'began',\n",
       " 'work',\n",
       " 'hollywood',\n",
       " 'producer',\n",
       " 'death',\n",
       " 'roles',\n",
       " 'aircraft',\n",
       " 'air',\n",
       " 'fighter',\n",
       " 'airbus',\n",
       " 'new',\n",
       " 'development',\n",
       " 'systems',\n",
       " 'united',\n",
       " 'boeing',\n",
       " 'bae',\n",
       " 'company',\n",
       " 'service',\n",
       " 'defence',\n",
       " 'jet',\n",
       " 'eurofighter',\n",
       " 'combat',\n",
       " 'flight',\n",
       " 'sixthgeneration',\n",
       " 'family',\n",
       " 'russian',\n",
       " 'ethiopian',\n",
       " 'typhoon',\n",
       " 'engine',\n",
       " 'british',\n",
       " 'dassault',\n",
       " 'generation',\n",
       " 'future',\n",
       " 'version',\n",
       " 'designed',\n",
       " 'market',\n",
       " 'film',\n",
       " 'love',\n",
       " 'species',\n",
       " 'whales',\n",
       " 'right',\n",
       " 'whale',\n",
       " 'united',\n",
       " 'pacific',\n",
       " 'mauritius',\n",
       " 'parakeet',\n",
       " 'north',\n",
       " 'women',\n",
       " 'series',\n",
       " 'including',\n",
       " 'population',\n",
       " 'states',\n",
       " 'american',\n",
       " 'family',\n",
       " 'islands',\n",
       " 'actress',\n",
       " 'best',\n",
       " 'novel',\n",
       " 'world',\n",
       " 'arabian',\n",
       " 'crocodiles',\n",
       " 'smith',\n",
       " 'born',\n",
       " 'known',\n",
       " 'philosophy',\n",
       " 'award',\n",
       " 'series',\n",
       " 'television',\n",
       " 'born',\n",
       " 'film',\n",
       " 'american',\n",
       " 'known',\n",
       " 'comics',\n",
       " 'dc',\n",
       " 'boys',\n",
       " 'new',\n",
       " 'book',\n",
       " 'roles',\n",
       " 'comic',\n",
       " 'actor',\n",
       " 'bachelor',\n",
       " 'hotel',\n",
       " 'role',\n",
       " 'marvel',\n",
       " 'issue',\n",
       " 'best',\n",
       " 'model',\n",
       " 'abc',\n",
       " 'july',\n",
       " 'appeared',\n",
       " 'host',\n",
       " 'amazon',\n",
       " 'world',\n",
       " 'actress',\n",
       " 'drama',\n",
       " 'illustrated',\n",
       " 'game',\n",
       " 'games',\n",
       " 'video',\n",
       " 'released',\n",
       " 'rare',\n",
       " 'spectrum',\n",
       " 'ultimate',\n",
       " 'graphics',\n",
       " 'group',\n",
       " 'computer',\n",
       " 'series',\n",
       " 'knight',\n",
       " 'song',\n",
       " 'zx',\n",
       " 'united',\n",
       " 'wrestling',\n",
       " 'white',\n",
       " 'magazine',\n",
       " 'company',\n",
       " 'american',\n",
       " 'including',\n",
       " 'new',\n",
       " 'born',\n",
       " 'isometric',\n",
       " 'vikernes',\n",
       " 'later',\n",
       " 'championship',\n",
       " 'lore',\n",
       " 'xbox',\n",
       " 'bbc',\n",
       " 'prints',\n",
       " 'century',\n",
       " 'printing',\n",
       " 'ukiyoe',\n",
       " 'museum',\n",
       " 'series',\n",
       " 'british',\n",
       " 'japanese',\n",
       " 'art',\n",
       " 'mirror',\n",
       " 'games',\n",
       " 'helmet',\n",
       " 'east',\n",
       " 'used',\n",
       " 'fuji',\n",
       " 'loot',\n",
       " 'surface',\n",
       " 'views',\n",
       " 'works',\n",
       " 'known',\n",
       " 'artists',\n",
       " 'hiroshiges',\n",
       " 'mount',\n",
       " 'woodblock',\n",
       " 'sutton',\n",
       " 'western',\n",
       " 'hokusai',\n",
       " 'maryon',\n",
       " 'metalwork',\n",
       " 'widely',\n",
       " 'album',\n",
       " 'american',\n",
       " 'united',\n",
       " 'born',\n",
       " 'released',\n",
       " 'music',\n",
       " 'series',\n",
       " 'band',\n",
       " 'megadeth',\n",
       " 'states',\n",
       " 'sexual',\n",
       " 'clarkson',\n",
       " 'number',\n",
       " 'billboard',\n",
       " 'studio',\n",
       " 'security',\n",
       " 'following',\n",
       " 'albums',\n",
       " 'new',\n",
       " 'women',\n",
       " 'guitarist',\n",
       " 'games',\n",
       " 'metal',\n",
       " 'hunger',\n",
       " 'records',\n",
       " 'federal',\n",
       " 'grammy',\n",
       " 'pop',\n",
       " 'chart',\n",
       " 'mustaine',\n",
       " 'united',\n",
       " 'iran',\n",
       " 'saudi',\n",
       " 'states',\n",
       " 'property',\n",
       " 'government',\n",
       " 'iranian',\n",
       " 'sanctions',\n",
       " 'president',\n",
       " 'international',\n",
       " 'khashoggi',\n",
       " 'fsln',\n",
       " 'war',\n",
       " 'film',\n",
       " 'waters',\n",
       " 'including',\n",
       " 'national',\n",
       " 'foreign',\n",
       " 'consulate',\n",
       " 'irans',\n",
       " 'american',\n",
       " 'reagan',\n",
       " 'gulf',\n",
       " 'november',\n",
       " 'october',\n",
       " 'nicaragua',\n",
       " 'action',\n",
       " 'high',\n",
       " 'contras',\n",
       " 'persian',\n",
       " 'american',\n",
       " 'film',\n",
       " 'hydrogen',\n",
       " 'series',\n",
       " 'television',\n",
       " 'born',\n",
       " 'united',\n",
       " 'comedy',\n",
       " 'city',\n",
       " 'states',\n",
       " 'landis',\n",
       " 'gas',\n",
       " 'known',\n",
       " 'writing',\n",
       " 'disease',\n",
       " 'rail',\n",
       " 'february',\n",
       " 'murphy',\n",
       " 'years',\n",
       " 'directed',\n",
       " 'based',\n",
       " 'energy',\n",
       " 'gases',\n",
       " 'people',\n",
       " 'world',\n",
       " 'actor',\n",
       " 'new',\n",
       " 'written',\n",
       " 'action',\n",
       " 'dysgraphia',\n",
       " 'dynasty',\n",
       " 'chinese',\n",
       " 'emperor',\n",
       " 'tang',\n",
       " 'han',\n",
       " 'li',\n",
       " 'nanyue',\n",
       " 'china',\n",
       " 'sui',\n",
       " 'jade',\n",
       " 'country',\n",
       " 'zhao',\n",
       " 'bc',\n",
       " 'including',\n",
       " 'population',\n",
       " 'taizong',\n",
       " 'central',\n",
       " 'somalia',\n",
       " 'regions',\n",
       " 'international',\n",
       " 'southern',\n",
       " 'empire',\n",
       " 'northern',\n",
       " 'century',\n",
       " 'somali',\n",
       " 'established',\n",
       " 'languages',\n",
       " 'nicaragua',\n",
       " 'kingdom',\n",
       " 'yang',\n",
       " 'film',\n",
       " 'series',\n",
       " 'actress',\n",
       " 'american',\n",
       " 'born',\n",
       " 'films',\n",
       " 'awards',\n",
       " 'television',\n",
       " 'award',\n",
       " 'actor',\n",
       " 'known',\n",
       " 'roles',\n",
       " 'best',\n",
       " 'peewee',\n",
       " 'reubens',\n",
       " 'april',\n",
       " 'role',\n",
       " 'scoobydoo',\n",
       " 'comedy',\n",
       " 'drama',\n",
       " 'debut',\n",
       " 'million',\n",
       " 'media',\n",
       " 'began',\n",
       " 'produced',\n",
       " 'fashion',\n",
       " 'music',\n",
       " 'spanish',\n",
       " 'divergent',\n",
       " 'including',\n",
       " 'aircraft',\n",
       " 'air',\n",
       " 'force',\n",
       " 'military',\n",
       " 'area',\n",
       " 'states',\n",
       " 'united',\n",
       " 'aerial',\n",
       " 'iran',\n",
       " 'unmanned',\n",
       " 'war',\n",
       " 'command',\n",
       " 'defence',\n",
       " 'systems',\n",
       " 'iranian',\n",
       " 'control',\n",
       " 'reconnaissance',\n",
       " 'soviet',\n",
       " 'forces',\n",
       " 'defense',\n",
       " 'surveillance',\n",
       " 'raaf',\n",
       " 'vehicle',\n",
       " 'flight',\n",
       " 'operations',\n",
       " 'used',\n",
       " 'uavs',\n",
       " 'islamic',\n",
       " 'missiles',\n",
       " 'based',\n",
       " 'egyptian',\n",
       " 'president',\n",
       " 'egypt',\n",
       " 'morsi',\n",
       " 'military',\n",
       " 'june',\n",
       " 'arabic',\n",
       " 'minister',\n",
       " 'election',\n",
       " 'mohamed',\n",
       " 'arab',\n",
       " 'french',\n",
       " 'war',\n",
       " 'mubarak',\n",
       " 'muslim',\n",
       " 'born',\n",
       " 'brotherhood',\n",
       " 'government',\n",
       " 'served',\n",
       " 'presidential',\n",
       " 'prime',\n",
       " 'political',\n",
       " 'july',\n",
       " 'august',\n",
       " 'egypts',\n",
       " 'church',\n",
       " 'france',\n",
       " 'office',\n",
       " 'party',\n",
       " 'coup',\n",
       " 'film',\n",
       " 'american',\n",
       " 'united',\n",
       " 'states',\n",
       " 'award',\n",
       " 'album',\n",
       " 'released',\n",
       " 'library',\n",
       " 'series',\n",
       " 'known',\n",
       " 'best',\n",
       " 'national',\n",
       " 'born',\n",
       " 'actor',\n",
       " 'music',\n",
       " 'received',\n",
       " 'books',\n",
       " 'discworld',\n",
       " 'world',\n",
       " 'albums',\n",
       " 'novels',\n",
       " 'june',\n",
       " 'imam',\n",
       " 'death',\n",
       " 'nominated',\n",
       " 'awards',\n",
       " 'congress',\n",
       " 'space',\n",
       " 'television',\n",
       " 'million']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the most common top words --> add them to the stop word list\n",
    "from collections import Counter\n",
    "\n",
    "# Let's first pull out the top 30 words for each cluster\n",
    "words = []\n",
    "for cluster in data_dtm.transpose().columns:\n",
    "    top = [word for (word, count) in top_dict[cluster]]\n",
    "    for t in top:\n",
    "        words.append(t)\n",
    "        \n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If more than half of the cluster have it as a top word, exclude it from the list\n",
    "add_stop_words = [word for word, count in Counter(words).most_common() if count > len(df)]\n",
    "add_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's update our document-term matrix with the new list of stop words\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Read in cleaned data\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "\n",
    "# Add new stop words\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate document-term matrix\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "data_cv = cv.fit_transform(data_clean.extract)\n",
    "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_stop.index = data_clean.index\n",
    "\n",
    "# Pickle it for later use\n",
    "import pickle\n",
    "pickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))\n",
    "data_stop.to_pickle(\"dtm_stop.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Attempt : all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aardvark</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaron</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ab</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ababa</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  \\\n",
       "aardvark   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0   0   \n",
       "aaron      0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "ab         0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "ababa      0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   \n",
       "abandon    0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   \n",
       "\n",
       "          17  \n",
       "aardvark   0  \n",
       "aaron      0  \n",
       "ab         0  \n",
       "ababa      0  \n",
       "abandon    0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data_dtm.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"air\" + 0.006*\"aircraft\" + 0.005*\"united\" + 0.005*\"game\" + 0.004*\"states\" + 0.004*\"american\" + 0.003*\"force\" + 0.003*\"games\" + 0.003*\"military\" + 0.003*\"series\"'),\n",
       " (1,\n",
       "  '0.010*\"series\" + 0.005*\"television\" + 0.004*\"born\" + 0.004*\"american\" + 0.004*\"film\" + 0.004*\"known\" + 0.003*\"dc\" + 0.003*\"comics\" + 0.003*\"boys\" + 0.002*\"new\"'),\n",
       " (2,\n",
       "  '0.007*\"american\" + 0.007*\"film\" + 0.005*\"born\" + 0.004*\"united\" + 0.004*\"series\" + 0.003*\"states\" + 0.003*\"new\" + 0.003*\"known\" + 0.003*\"best\" + 0.003*\"vanderbilt\"'),\n",
       " (3,\n",
       "  '0.005*\"united\" + 0.004*\"aircraft\" + 0.004*\"air\" + 0.003*\"states\" + 0.003*\"saudi\" + 0.003*\"iran\" + 0.003*\"war\" + 0.003*\"british\" + 0.002*\"government\" + 0.002*\"new\"'),\n",
       " (4,\n",
       "  '0.010*\"dynasty\" + 0.008*\"chinese\" + 0.006*\"emperor\" + 0.006*\"tang\" + 0.005*\"han\" + 0.004*\"nanyue\" + 0.004*\"li\" + 0.003*\"china\" + 0.003*\"sui\" + 0.003*\"jade\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=5, passes=40)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Attempt : nouns only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saintnazaire french pronunciation  breton sant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gadsby is a  novel by ernest vincent wright wr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bombshell is a  american precode romantic come...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patrick michael shanahan born june   is an ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>west side story is a  american romantic musica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the mikoyan  russian   nato reporting name ful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the witches is a  dark fantasy comedy film dir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>samara weaving born  february  is an australia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>elite is a space trading video game  it was wr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>thirtysix views of mount fuji  fugaku sanjūrok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>last week tonight with john oliver often abrid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>there have been a number of sanctions against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>deborah nadoolman landis born may   is an amer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jade refers to an ornamental mineral mostly kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bruce weigert paltrow november    october   wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the  english thunderbolt is an iranian turbofa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>entissar amer arabic   born  december  is marr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>kelendria trene kelly  rowland born february  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              extract\n",
       "0   saintnazaire french pronunciation  breton sant...\n",
       "1   gadsby is a  novel by ernest vincent wright wr...\n",
       "2   bombshell is a  american precode romantic come...\n",
       "3   patrick michael shanahan born june   is an ame...\n",
       "4   west side story is a  american romantic musica...\n",
       "5   the mikoyan  russian   nato reporting name ful...\n",
       "6   the witches is a  dark fantasy comedy film dir...\n",
       "7   samara weaving born  february  is an australia...\n",
       "8   elite is a space trading video game  it was wr...\n",
       "9   thirtysix views of mount fuji  fugaku sanjūrok...\n",
       "10  last week tonight with john oliver often abrid...\n",
       "11  there have been a number of sanctions against ...\n",
       "12  deborah nadoolman landis born may   is an amer...\n",
       "13  jade refers to an ornamental mineral mostly kn...\n",
       "14  bruce weigert paltrow november    october   wa...\n",
       "15  the  english thunderbolt is an iranian turbofa...\n",
       "16  entissar amer arabic   born  december  is marr...\n",
       "17  kelendria trene kelly  rowland born february  ..."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>french neñseir western traditional major right...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>novel ernest fictional little noticed constrai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>american romantic jean morgan c mary franchot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>american retired general white mark last june ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>west american romantic musical robert broadway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mikoyan russian russian united further first s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dark anjelica rowan same original evil ordinar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>born australian further indi australian ready ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ian acorn september revolutionary contemporary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>thirtysix fugaku japanese ukiyoe mount differe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>last last american comedian halfhourlong sunda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>united international first united radical amer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nadoolman american such notable animal more ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ornamental green different ancient asian impor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>paltrow american american musician comedian vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>english iranian unmanned aerial shahed smaller...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>entissar born sixth egypt first el arabic egyp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>born american late hiatus first deep uk single...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              extract\n",
       "0   french neñseir western traditional major right...\n",
       "1   novel ernest fictional little noticed constrai...\n",
       "2   american romantic jean morgan c mary franchot ...\n",
       "3   american retired general white mark last june ...\n",
       "4   west american romantic musical robert broadway...\n",
       "5   mikoyan russian russian united further first s...\n",
       "6   dark anjelica rowan same original evil ordinar...\n",
       "7   born australian further indi australian ready ...\n",
       "8   ian acorn september revolutionary contemporary...\n",
       "9   thirtysix fugaku japanese ukiyoe mount differe...\n",
       "10  last last american comedian halfhourlong sunda...\n",
       "11  united international first united radical amer...\n",
       "12  nadoolman american such notable animal more ma...\n",
       "13  ornamental green different ancient asian impor...\n",
       "14  paltrow american american musician comedian vi...\n",
       "15  english iranian unmanned aerial shahed smaller...\n",
       "16  entissar born sixth egypt first el arabic egyp...\n",
       "17  born american late hiatus first deep uk single..."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.extract.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aardvark</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>ababa</th>\n",
       "      <th>abba</th>\n",
       "      <th>abbas</th>\n",
       "      <th>abbasid</th>\n",
       "      <th>abbassia</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>...</th>\n",
       "      <th>éomer</th>\n",
       "      <th>ögedei</th>\n",
       "      <th>āṣifat</th>\n",
       "      <th>īsā</th>\n",
       "      <th>ōban</th>\n",
       "      <th>ḥiṣān</th>\n",
       "      <th>ḥusayn</th>\n",
       "      <th>ḥusnī</th>\n",
       "      <th>ḥusēn</th>\n",
       "      <th>ḵāšuqjī</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows × 8548 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aardvark  aaron  ab  ababa  abba  abbas  abbasid  abbassia  abbey  \\\n",
       "0          0      0   0      0     0      0        0         0      2   \n",
       "1          0      1   1      0     0      0        0         0      0   \n",
       "2          0      0   0      0     0      0        0         0      0   \n",
       "3          0      0   0      0     0      0        0         0      0   \n",
       "4          0      0   0      0     0      0        0         0      0   \n",
       "5          0      0   0      1     0      0        0         0      0   \n",
       "6          0      0   0      0     0      0        0         0      0   \n",
       "7          0      0   0      0     0      0        0         0      0   \n",
       "8          0      0   0      0     0      0        0         0      0   \n",
       "9          0      0   0      0     0      0        0         0      0   \n",
       "10         0      0   0      0     0      0        0         0      0   \n",
       "11         0      0   0      0     0      1        0         0      0   \n",
       "12         0      0   0      0     0      0        0         0      0   \n",
       "13         0      0   0      0     0      0        0         0      0   \n",
       "14         2      0   0      0     0      0        0         0      0   \n",
       "15         0      0   0      0     0      0        0         0      0   \n",
       "16         0      0   0      0     1      0        0         1      0   \n",
       "17         0      0   0      0     0      0        1         0      0   \n",
       "\n",
       "    abbreviation  ...  éomer  ögedei  āṣifat  īsā  ōban  ḥiṣān  ḥusayn  ḥusnī  \\\n",
       "0              0  ...      0       0       0    0     0      0       0      0   \n",
       "1              0  ...      0       0       0    0     0      0       0      0   \n",
       "2              0  ...      0       0       0    0     0      0       0      0   \n",
       "3              0  ...      0       0       0    0     0      0       0      0   \n",
       "4              0  ...      0       0       0    0     0      0       0      0   \n",
       "5              0  ...      0       0       0    0     0      0       0      0   \n",
       "6              0  ...      0       0       0    0     0      1       0      0   \n",
       "7              0  ...      1       0       0    0     0      0       0      0   \n",
       "8              1  ...      0       0       0    0     0      0       0      0   \n",
       "9              0  ...      0       0       0    0     1      0       0      0   \n",
       "10             0  ...      0       0       0    0     0      0       0      0   \n",
       "11             0  ...      0       0       1    0     0      0       0      0   \n",
       "12             0  ...      0       0       0    0     0      0       0      0   \n",
       "13             0  ...      0       0       0    0     0      0       0      0   \n",
       "14             0  ...      0       0       0    0     0      0       0      0   \n",
       "15             0  ...      0       0       0    0     0      0       0      0   \n",
       "16             0  ...      0       0       0    1     0      0       1      1   \n",
       "17             0  ...      0       1       0    0     0      0       0      0   \n",
       "\n",
       "    ḥusēn  ḵāšuqjī  \n",
       "0       0        0  \n",
       "1       0        0  \n",
       "2       0        0  \n",
       "3       0        0  \n",
       "4       0        0  \n",
       "5       0        0  \n",
       "6       0        0  \n",
       "7       0        0  \n",
       "8       0        0  \n",
       "9       0        0  \n",
       "10      0        0  \n",
       "11      0        1  \n",
       "12      0        0  \n",
       "13      0        0  \n",
       "14      0        0  \n",
       "15      0        0  \n",
       "16      1        0  \n",
       "17      0        0  \n",
       "\n",
       "[18 rows x 8548 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.extract)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"president\" + 0.005*\"war\" + 0.004*\"states\" + 0.004*\"government\" + 0.003*\"world\" + 0.003*\"june\" + 0.003*\"city\" + 0.003*\"france\" + 0.003*\"egypt\" + 0.003*\"dynasty\"'),\n",
       " (1,\n",
       "  '0.010*\"film\" + 0.008*\"series\" + 0.005*\"states\" + 0.005*\"television\" + 0.005*\"aircraft\" + 0.005*\"air\" + 0.004*\"films\" + 0.003*\"award\" + 0.003*\"family\" + 0.003*\"album\"')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"city\" + 0.005*\"championship\" + 0.005*\"war\" + 0.004*\"france\" + 0.004*\"battle\" + 0.004*\"army\" + 0.004*\"world\" + 0.003*\"secretary\" + 0.003*\"years\" + 0.003*\"year\"'),\n",
       " (1,\n",
       "  '0.015*\"film\" + 0.011*\"series\" + 0.007*\"television\" + 0.005*\"films\" + 0.005*\"states\" + 0.005*\"award\" + 0.005*\"actress\" + 0.005*\"album\" + 0.004*\"actor\" + 0.004*\"role\"'),\n",
       " (2,\n",
       "  '0.008*\"game\" + 0.006*\"games\" + 0.005*\"vanderbilt\" + 0.005*\"family\" + 0.005*\"century\" + 0.004*\"dynasty\" + 0.004*\"emperor\" + 0.004*\"series\" + 0.004*\"york\" + 0.003*\"tang\"'),\n",
       " (3,\n",
       "  '0.012*\"aircraft\" + 0.012*\"air\" + 0.008*\"president\" + 0.008*\"states\" + 0.006*\"war\" + 0.005*\"iran\" + 0.005*\"government\" + 0.005*\"force\" + 0.004*\"egypt\" + 0.004*\"systems\"')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with more topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Attempt : nouns and adjectives only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saintnazaire french pronunciation breton neñse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gadsby novel ernest vincent wright lipogram wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bombshell american precode romantic comedydram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patrick michael june american government offic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>west side story american romantic musical dram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mikoyan russian nato reporting name fulcrumf r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>witches dark fantasy comedy film nicolas roeg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>samara born february australian actress model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>elite space trading video game david braben ia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>thirtysix views mount fuji fugaku sanjūrokkei ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>last week tonight john last week tonight ameri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>number sanctions iran number countries united ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>deborah nadoolman born american author histori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jade refers ornamental mineral green varieties...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bruce weigert paltrow october american televis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>english thunderbolt iranian wing unmanned comb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>entissar amer arabic born december sixth presi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>kelendria born february american singer songwr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              extract\n",
       "0   saintnazaire french pronunciation breton neñse...\n",
       "1   gadsby novel ernest vincent wright lipogram wo...\n",
       "2   bombshell american precode romantic comedydram...\n",
       "3   patrick michael june american government offic...\n",
       "4   west side story american romantic musical dram...\n",
       "5   mikoyan russian nato reporting name fulcrumf r...\n",
       "6   witches dark fantasy comedy film nicolas roeg ...\n",
       "7   samara born february australian actress model ...\n",
       "8   elite space trading video game david braben ia...\n",
       "9   thirtysix views mount fuji fugaku sanjūrokkei ...\n",
       "10  last week tonight john last week tonight ameri...\n",
       "11  number sanctions iran number countries united ...\n",
       "12  deborah nadoolman born american author histori...\n",
       "13  jade refers ornamental mineral green varieties...\n",
       "14  bruce weigert paltrow october american televis...\n",
       "15  english thunderbolt iranian wing unmanned comb...\n",
       "16  entissar amer arabic born december sixth presi...\n",
       "17  kelendria born february american singer songwr..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.extract.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aardvark</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>ababa</th>\n",
       "      <th>abba</th>\n",
       "      <th>abbas</th>\n",
       "      <th>abbasid</th>\n",
       "      <th>abbassia</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>...</th>\n",
       "      <th>ögedei</th>\n",
       "      <th>āṣifat</th>\n",
       "      <th>īsā</th>\n",
       "      <th>ōban</th>\n",
       "      <th>ḥizb</th>\n",
       "      <th>ḥiṣān</th>\n",
       "      <th>ḥusayn</th>\n",
       "      <th>ḥusnī</th>\n",
       "      <th>ḥusēn</th>\n",
       "      <th>ḵāšuqjī</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows × 10644 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aardvark  aaron  ab  ababa  abba  abbas  abbasid  abbassia  abbey  \\\n",
       "0          0      0   0      0     0      0        0         0      2   \n",
       "1          0      1   1      0     0      0        0         0      0   \n",
       "2          0      0   0      0     0      0        0         0      0   \n",
       "3          0      0   0      0     0      0        0         0      0   \n",
       "4          0      0   0      0     0      0        0         0      0   \n",
       "5          0      0   0      1     0      0        0         0      0   \n",
       "6          0      0   0      0     0      0        0         0      0   \n",
       "7          0      0   0      0     0      0        0         0      0   \n",
       "8          0      0   0      0     0      0        0         0      0   \n",
       "9          0      0   0      0     0      0        0         0      0   \n",
       "10         0      0   0      0     0      0        0         0      0   \n",
       "11         0      0   0      0     0      1        0         0      0   \n",
       "12         0      0   0      0     0      0        0         0      0   \n",
       "13         0      0   0      0     0      0        0         0      0   \n",
       "14         2      0   0      0     0      0        0         0      0   \n",
       "15         0      0   0      0     0      0        0         0      0   \n",
       "16         0      0   0      0     1      0        0         1      0   \n",
       "17         0      0   0      0     0      0        1         0      0   \n",
       "\n",
       "    abbreviation  ...  ögedei  āṣifat  īsā  ōban  ḥizb  ḥiṣān  ḥusayn  ḥusnī  \\\n",
       "0              0  ...       0       0    0     0     0      0       0      0   \n",
       "1              0  ...       0       0    0     0     0      0       0      0   \n",
       "2              0  ...       0       0    0     0     0      0       0      0   \n",
       "3              0  ...       0       0    0     0     0      0       0      0   \n",
       "4              0  ...       0       0    0     0     0      0       0      0   \n",
       "5              0  ...       0       0    0     0     0      0       0      0   \n",
       "6              0  ...       0       0    0     0     0      1       0      0   \n",
       "7              0  ...       0       0    0     0     0      0       0      0   \n",
       "8              1  ...       0       0    0     0     0      0       0      0   \n",
       "9              0  ...       0       0    0     1     0      0       0      0   \n",
       "10             0  ...       0       0    0     0     0      0       0      0   \n",
       "11             0  ...       0       1    0     0     0      0       0      0   \n",
       "12             0  ...       0       0    0     0     0      0       0      0   \n",
       "13             0  ...       0       0    0     0     0      0       0      0   \n",
       "14             0  ...       0       0    0     0     0      0       0      0   \n",
       "15             0  ...       0       0    0     0     0      0       0      0   \n",
       "16             0  ...       0       0    1     0     1      0       1      1   \n",
       "17             0  ...       1       0    0     0     0      0       0      0   \n",
       "\n",
       "    ḥusēn  ḵāšuqjī  \n",
       "0       0        0  \n",
       "1       0        0  \n",
       "2       0        0  \n",
       "3       0        0  \n",
       "4       0        0  \n",
       "5       0        0  \n",
       "6       0        0  \n",
       "7       0        0  \n",
       "8       0        0  \n",
       "9       0        0  \n",
       "10      0        0  \n",
       "11      0        1  \n",
       "12      0        0  \n",
       "13      0        0  \n",
       "14      0        0  \n",
       "15      0        0  \n",
       "16      1        0  \n",
       "17      0        0  \n",
       "\n",
       "[18 rows x 10644 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.extract)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"film\" + 0.004*\"television\" + 0.003*\"films\" + 0.003*\"family\" + 0.003*\"vanderbilt\" + 0.003*\"award\" + 0.003*\"game\" + 0.003*\"actress\" + 0.002*\"actor\" + 0.002*\"best\"'),\n",
       " (1,\n",
       "  '0.004*\"president\" + 0.004*\"air\" + 0.004*\"military\" + 0.003*\"aircraft\" + 0.003*\"iran\" + 0.003*\"area\" + 0.003*\"egyptian\" + 0.003*\"government\" + 0.002*\"film\" + 0.002*\"egypt\"')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"vanderbilt\" + 0.006*\"game\" + 0.005*\"film\" + 0.005*\"family\" + 0.004*\"games\" + 0.003*\"york\" + 0.003*\"right\" + 0.003*\"species\" + 0.002*\"whales\" + 0.002*\"video\"'),\n",
       " (1,\n",
       "  '0.009*\"album\" + 0.004*\"music\" + 0.004*\"band\" + 0.003*\"sexual\" + 0.003*\"studio\" + 0.003*\"albums\" + 0.003*\"security\" + 0.003*\"clarkson\" + 0.003*\"women\" + 0.003*\"guitarist\"'),\n",
       " (2,\n",
       "  '0.013*\"film\" + 0.006*\"television\" + 0.005*\"films\" + 0.004*\"actor\" + 0.004*\"award\" + 0.004*\"actress\" + 0.003*\"roles\" + 0.003*\"awards\" + 0.003*\"career\" + 0.003*\"best\"'),\n",
       " (3,\n",
       "  '0.007*\"air\" + 0.006*\"aircraft\" + 0.005*\"military\" + 0.005*\"president\" + 0.003*\"government\" + 0.003*\"iran\" + 0.003*\"french\" + 0.003*\"egyptian\" + 0.003*\"force\" + 0.003*\"area\"')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"saudi\" + 0.006*\"iran\" + 0.004*\"government\" + 0.004*\"property\" + 0.004*\"iranian\" + 0.004*\"president\" + 0.004*\"sanctions\" + 0.003*\"prints\" + 0.003*\"fsln\" + 0.003*\"khashoggi\"'),\n",
       " (1,\n",
       "  '0.008*\"vanderbilt\" + 0.007*\"album\" + 0.006*\"film\" + 0.004*\"music\" + 0.004*\"york\" + 0.004*\"award\" + 0.003*\"family\" + 0.003*\"albums\" + 0.003*\"actor\" + 0.003*\"band\"'),\n",
       " (2,\n",
       "  '0.009*\"film\" + 0.006*\"species\" + 0.006*\"right\" + 0.006*\"whales\" + 0.005*\"love\" + 0.004*\"whale\" + 0.004*\"pacific\" + 0.003*\"north\" + 0.003*\"parakeet\" + 0.003*\"women\"'),\n",
       " (3,\n",
       "  '0.005*\"championship\" + 0.005*\"french\" + 0.004*\"france\" + 0.004*\"battle\" + 0.004*\"wwe\" + 0.003*\"army\" + 0.003*\"title\" + 0.003*\"scientology\" + 0.003*\"champion\" + 0.003*\"death\"'),\n",
       " (4,\n",
       "  '0.011*\"film\" + 0.009*\"aircraft\" + 0.008*\"air\" + 0.006*\"television\" + 0.005*\"films\" + 0.004*\"actress\" + 0.004*\"force\" + 0.003*\"actor\" + 0.003*\"roles\" + 0.003*\"systems\"'),\n",
       " (5,\n",
       "  '0.009*\"egyptian\" + 0.009*\"president\" + 0.009*\"game\" + 0.008*\"egypt\" + 0.006*\"games\" + 0.006*\"morsi\" + 0.005*\"military\" + 0.004*\"election\" + 0.004*\"minister\" + 0.004*\"video\"'),\n",
       " (6,\n",
       "  '0.013*\"dynasty\" + 0.012*\"chinese\" + 0.009*\"emperor\" + 0.008*\"tang\" + 0.007*\"han\" + 0.005*\"li\" + 0.005*\"nanyue\" + 0.004*\"china\" + 0.004*\"sui\" + 0.004*\"jade\"')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=7, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify topics in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "#list(zip([a for [(a,b)] in corpus_transformed], data_dtm.index))\n",
    "try:\n",
    "    listt = list(zip(corpus_transformed, data_dtm.index))\n",
    "except IndexError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 0 Topic(s): [3]\n",
      "Class: 1 Topic(s): [3]\n",
      "Class: 2 Topic(s): [1]\n",
      "Class: 3 Topic(s): [4]\n",
      "Class: 4 Topic(s): [4]\n",
      "Class: 5 Topic(s): [4]\n",
      "Class: 6 Topic(s): [2]\n",
      "Class: 7 Topic(s): [4]\n",
      "Class: 8 Topic(s): [5]\n",
      "Class: 9 Topic(s): [0]\n",
      "Class: 10 Topic(s): [1]\n",
      "Class: 11 Topic(s): [0]\n",
      "Class: 12 Topic(s): [4]\n",
      "Class: 13 Topic(s): [6]\n",
      "Class: 14 Topic(s): [4]\n",
      "Class: 15 Topic(s): [4]\n",
      "Class: 16 Topic(s): [5]\n",
      "Class: 17 Topic(s): [1]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(listt)):\n",
    "    list_of_topics = []\n",
    "    for j in range(0, len(listt[i][0])):\n",
    "        list_of_topics.append(listt[i][0][j][0])\n",
    "    print('Class:' , listt[i][1], 'Topic(s):', list_of_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
